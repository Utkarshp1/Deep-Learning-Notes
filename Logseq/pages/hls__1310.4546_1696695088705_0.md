file:: [1310.4546_1696695088705_0.pdf](../assets/1310.4546_1696695088705_0.pdf)
file-path:: ../assets/1310.4546_1696695088705_0.pdf

- The main advantage is that instead of evaluating W output nodes in the neural network to obtain the probability distribution, it is needed to evaluate only about log2(W ) nodes.
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 65219ab9-dbf5-463b-ad7d-7d7e1e8ded33
- The hierarchical softmax uses a binary tree representation of the output layer with the W words as its leaves and, for each node, explicitly represents the relative probabilities of its child nodes. These deﬁne a random walk that assigns probabilities to words
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 65219ac8-b56c-4329-b9d7-6137dba6a351
- [:span]
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 65219b3e-08de-4908-bd7b-85fa73bbe9fa
  hl-type:: area
  hl-stamp:: 1696701246221
- The structure of the tree used by the hierarchical softmax has a considerable effect on the performance. Mnih and Hinton explored a number of methods for constructing the tree structure and the effect on both the training time and the resulting model accuracy [10]. In our work we use a binary Huffman tree, as it assigns short codes to the frequent words which results in fast training. It has been observed before that grouping words together by their frequency works well as a very simple speedup technique for the neural network based language models [5, 8]
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 65219c30-866a-4c8a-b82a-3b8f33b2b050
- [:span]
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 6521a28a-23ba-4a8f-9668-5321295aaa76
  hl-type:: area
  hl-stamp:: 1696703113724
- [:span]
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 6521b11f-0f35-48ed-b797-c414dcc33c2d
  hl-type:: area
  hl-stamp:: 1696706847113
- [:span]
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 6521b133-8c36-4681-8014-fa1438dc33c1
  hl-type:: area
  hl-stamp:: 1696706866893
- To learn vector representation for phrases, we ﬁrst ﬁnd words that appear frequently together, and infrequently in other contexts. For example, “New York Times” and“Toronto Maple Leafs” are replaced by unique tokens in the training data, while a bigram “this is” will remain unchanged
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 6521b5f0-8f26-4f11-bd50-014ca84c43d1
- The bigrams with score above the chosen threshold are then used as phrases. Typically, we run 2-4 passes over the training data with decreasing threshold value, allowing longer phrases that consists of several words to be formed.
  ls-type:: annotation
  hl-page:: 6
  hl-color:: yellow
  id:: 6521ba4e-e5c2-418e-a8de-8ec8cb3b3a23
- The additive property of the vectors can be explained by inspecting the training objective. The word vectors are in a linear relationship with the inputs to the softmax nonlinearity. As the word vectors are trained to predict the surrounding words in the sentence, the vectors can be seen as representing the distribution of the context in which a word appears. These values are related logarithmically to the probabilities computed by the output layer, so the sum of two word vectors is related to the product of the two context distributions. The product works here as the AND function: words that are assigned high probabilities by both word vectors will have high probability, and the other words will have low probability. Thus, if “Volga River” appears frequently in the same sentence together with the words “Russian” and “river”, the sum of these two word vectors will result in such a feature vector that is close to the vector of “Volga River”.
  ls-type:: annotation
  hl-page:: 7
  hl-color:: yellow
  id:: 6521bac1-fe2c-4831-b368-1370085c41bc