- ![1503.03832.pdf](../assets/1503.03832_1689103697803_0.pdf)
- **Basic Idea**:
	- To be able to directly train a DCNN to optimise the embedding itself, rather than an intermediate bottleneck layer (directly optimises the loss for the task at hand i.e., face verification).
	- The networks without 2D or 3D alignment of face images (only scale and translation is fixed).
	- ((64adaff1-f2b4-4c65-baf9-515cc2ab3efd))
	-
- **Different problems related to Faces in deep learning**:
	- *Face verification*: Given two images, is it the same person.
	- *Face recognition*: Who is this person.
	- *Face clustering*: Find common people among these faces.
	- ((64adaf0f-654b-4c5f-a10d-5c0a8e34856c))
	-
- **Triplet Loss**
	- Notation: $f(x) \in \mathbb{R}^{d}$ embedding of an image $x$ into a $d$-dimensional space.
	- Constrain the embedding to live on the $d$-dimensional hypersphere, i.e., $||f(x)||_{2} = 1$.
	- Motivation: We want to ensure that an image $x_{i}^{a}$ (anchor) of a specific person is closer to all other images $x_{p}^{i}$ (positive) of the same person than it is to any image $x_{n}^{i}$ (negative) of any other person. Thus we want,
	  id:: 64adb0d9-0ba4-404b-98bb-e4b0f9ac26c6
	  \begin{equation}
	  ||f(x_{i}^{a}) - f(x_{i}^{p})||_{2}^{2} + \alpha < ||f(x_{i}^{a}) - f(x_{i}^{n})||_{2}^{2} \\
	  \forall f(x_{i}^{a}), f(x_{i}^{p}), f(x_{i}^{n}) \in \mathcal{T}
	  \end{equation}
	  where $\alpha$ is a margin that is enforced between positive and negative pairs. $\mathcal{T}$ is the set of all possible triplets in the training set and has cardinality N.
	- The loss to minimise is:
	  \begin{equation}
	  \sum_{i}^{N}\left [ ||f(x_{i}^{a}) - f(x_{i}^{p})||_{2}^{2} - ||f(x_{i}^{a}) - f(x_{i}^{n})||_{2}^{2} + \alpha\right]_{+}
	  \end{equation}
	  where $[x]_{+} = max\{0, x\}$.
	- **Utility of margin**:
	  id:: 64adb328-e769-41a4-ad1d-24858e6722a8
		- At least, $\alpha$ is the distance between positive and negative examples (ensuring discriminability rather than just separability). The magnitude of $\alpha$ controls the amount of discriminative power of the embedding.
		- The role of the hinge function $[m + \cdot]_{+}$ is to avoid correcting "already correct" triplet because the triplets which already [satisfy](logseq://graph/Logseq?block-id=64adb0d9-0ba4-404b-98bb-e4b0f9ac26c6) will have loss L = 0. But for discriminability we also want the embeddings from the same class to be pulled close to each other, so this L should not be that easily satisfied i.e. $m$ should have sufficiently high value.
		-
- **Triplet Selection**
	- ((64b037ea-24bf-42d8-bd0d-930190fdb203))
	- This means that given $x_{i}^{a}$, we want to select an $x_{i}^{p}$ (hard positive) such that $argmax_{x_{i}^{p}}||f(x_{i}^{a}) - f(x_{i}^{p})||_{2}^{2}$ and similarly $x_{i}^{n}$ (hard negative) such that $argmin_{x_{i}^{n}}||f(x_{i}^{a}) - f(x_{i}^{n})||_{2}^{2}$.
	- ((64b0392f-82c3-4df8-aa06-26e5e3c1e153))
		- ((64b0393a-ce86-451e-ae9f-5b749587c7ac))
		- ((64b03947-ddba-4833-933e-d232c28e51b1))
	- Use online triplet generation. This requires two things:
		- Large batch-size so that argmin and argmax that are calculated are meaningful. Notice the trade-off: Smaller batch-sizes are required for faster convergence whereas larger batch-sizes required for contrastive training.
		- Minimal number of exemplars of one identity is present in each mini-batch so as to have meaningful representation of the anchor-positive distances (in this paper, they choose 40 images per identity per mini-batch and also add randomly sampled negative samples).
	- They use all anchor-positive pairs in a mini-batch for loss calculation instead of just using hardest positive examples.
	- Selecting the hardest negatives can in practice lead to bad local minima early on in training, specifically it can result in a collapsed model (i.e. $f(x) = 0$). In order to mitigate this, it helps to select $x^n_{i}$ such that
	  \begin{equation}
	  ||f(x_{i}^{a}) - f(x_{i}^{p})||_{2}^{2} < ||f(x_{i}^{a}) - f(x_{i}^{n})||_{2}^{2}
	  \end{equation}
	  These negative examples are called semi-hard because they are further away from the anchor than the the positive exemplar, but still hard because the squared distance is close to the anchor-positive distance. Ideally we would want no negative exemplar to be within $\alpha$ margin of the anchor. So, basically these semi-hard negative exemplars are all those negative exemplars which lie inside the margin $\alpha$ (for all other negative exemplar which satisfy the above condition but lie outside the margin will have loss L=0).
	- **Implementation**:
		- batch_size: 1800 exemplars
		- $\alpha$ : 0.2
	- #+BEGIN_CENTER
	  ((64bc1c3e-0857-464f-bb05-b223e128df4f))
	  #+END_CENTER
- **Harmonic Embedding**
	- Set of embeddings which are generated by different models but are still compatible with each other.
	- **Use-case**: ((64bc1ea2-9438-4ebf-8811-7cb0ac820ffb))
	- **Harmonic Triplet Loss**:
	- #+BEGIN_CENTER
	  ((64bc1f60-2aab-4f84-a9be-4a4be2bd3d56))
	  #+END_CENTER
	- #+BEGIN_CENTER
	  ((64bc1f07-2088-4711-bc3f-32eb80eb497f))
	  #+END_CENTER
- **Extensions: Deep Face Recognition by Parkhi et al. 2015**
	- Able to achieve the same performance as FaceNet but with considerably less data and less number of unique identities.
	- Instead of directly training the network with the triplet-loss, they first train the network using a classifier and then throw away the last layer. The training with a triplet-loss learns a new projection head whose output will be considered the embedding for the face.
	- **Sampling of triplets**: This is similar to the FaceNet (but more verbose). `An epoch here contains all the possible positive pairs` $(a, p)$, `where image` $a$ `is considered the anchor and` $p$ `its paired positive example. Choosing good triplets is crucial and should strike a balance between selecting informative (i.e. challenging) examples and swamping training with examples that are too hard. This is achieved by extending each pair` $(a. p)$ `to a triplet` $(a, p, n)$ `by sampling the image` $n$ `at random, but only between the ones that violate the triplet loss margin. The latter is a form of hard-negative mining, but it is not as aggressive as (and much cheaper than) choosing the maximally violating example, as often done in structured output learning.`
	- **Tricks of the trade**: The input to all networks is a face image of size 224 x 224 with the average face image (computed from the training set) subtracted - this is critical for the stability of the optimisation algorithm. Reason: If we are using ReLU activation, we want some portions of the inputs to be negative as well. Otherwise, there will be no non-linearity.
	- **Metrics**: In addition to the verification accuracy Acc., they use the *Equal Error Rate (EER)* as an evaluation metric, defined as the error rate at the ROC operating point where the false positive and false negative rates are equal. The advantage on Acc. is that it is (i.e., EER) independent of the distance threshold $\tau$. This is because there will be mostly one threshold where the false negatives and false positives rates would the same, whereas the accuracy is highly dependent on the threshold selection.
	-
-