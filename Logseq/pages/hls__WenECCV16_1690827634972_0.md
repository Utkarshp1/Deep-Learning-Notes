file:: [WenECCV16_1690827634972_0.pdf](../assets/WenECCV16_1690827634972_0.pdf)
file-path:: ../assets/WenECCV16_1690827634972_0.pdf

- [:span]
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 64c7fea2-3483-4264-a14d-c6e36007eb86
  hl-type:: area
  hl-stamp:: 1690828450312
- Discriminative power characterizes features in both the compact intra-class variations and separable inter-class differences, as shown in Fig. 1
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 64c7ff26-6924-426a-a59f-b12e2b2d3708
- However, the softmax loss only encourage the separability of features. The resulting features are not sufficiently effective for face recognition.
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 64c7ff42-93a2-4a6b-b3ba-c335cb6c5b1f
- Because the stochastic gradient descent (SGD) [19] optimizes the CNNs based on mini-batch, which can not reflect the global distribution of deep features very well. Due to the huge scale of training set, it is impractical to input all the training samples in every iteration. As alternative approaches, contrastive loss [10,29] and triplet loss [27] respectively construct loss functions for image pairs and triplet. 
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 64c7ff8e-a396-4142-ac77-76013ce02bfb
- he formulation effectively characterizes the intra-class variations. Ideally, the
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 64c80859-649c-4011-8cf6-7deeca099d8e
- should be updated as the deep features changed. In other words, we need to take the entire training set into account and average the features of every class in each iteration, which is inefficient even impractical. Therefore, the center loss can not be used directly.
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 64c8087b-8bcc-4da3-a83c-b991ce0085e6
- First, instead of updating the centers with respect to the entire training set, we perform the update based on mini-batch. In each iteration, the centers are computed by averaging the features of the corresponding classes (In this case, some of the centers may not update). 
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 64c8092c-1c62-4d2a-9e50-cb3c557244df
- Second, to avoid large perturbations caused by few mislabelled samples, we use a scalar Î± to control the learning rate of the centers.
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 64c80939-d8a1-4878-a245-7f57feeb7d1a
- [:span]
  ls-type:: annotation
  hl-page:: 6
  hl-color:: yellow
  id:: 64c811db-8a51-4230-a97f-6260423ea872
  hl-type:: area
  hl-stamp:: 1690833370747
- If we only use the softmax loss as supervision signal, the resulting deeply learned features would contain large intra-class variations. On the other hand, if we only supervise CNNs by the center loss, the deeply learned features and centers will degraded to zeros (At this point, the center loss is very small). 
  ls-type:: annotation
  hl-page:: 7
  hl-color:: yellow
  id:: 64d4638c-941f-4cbb-8584-d20ee409c776