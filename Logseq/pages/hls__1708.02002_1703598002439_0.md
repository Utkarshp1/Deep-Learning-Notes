file:: [1708.02002_1703598002439_0.pdf](../assets/1708.02002_1703598002439_0.pdf)
file-path:: ../assets/1708.02002_1703598002439_0.pdf

- ﬁxed foreground-to-background ratio (1:3), or online hard example mining (OHEM) [31], are performed to maintain a manageable balance between foreground and background.
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 658ad9c3-76e0-4f37-8ab7-e54afada92ac
- This inefﬁciency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping [33, 29] or hard example mining [37, 8, 31].
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 658ad9e6-2855-4ec8-9ffc-9055ed0898e5
- training is inefﬁcient as most locations are easy negatives that contribute no useful learning signal; 
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658adb37-f26d-45dc-a244-5a1f4a70eb5e
- en masse, the easy negatives can overwhelm training and lead to degenerate models
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658adb65-ec1f-48d2-9d53-d3e6862a52c6
- [:span]
  ls-type:: annotation
  hl-page:: 1
  hl-color:: yellow
  id:: 658b009b-90e6-4c6b-8dae-a709490d9ee3
  hl-type:: area
  hl-stamp:: 1703608475039
- [:span]
  ls-type:: annotation
  hl-page:: 7
  hl-color:: yellow
  id:: 658b0403-518e-41d4-91c2-c29be2417c45
  hl-type:: area
  hl-stamp:: 1703609346827
- One notable property of this loss, which can be easily seen in its plot, is that even examples that are easily classiﬁed (pt  .5) incur a loss with non-trivial magnitude. When summed over a large number of easy examples, these small loss values can overwhelm the rare class.
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658b0421-b9cc-48b6-a13b-5673922c3d0f
- In practice α may be set by inverse class frequency or treated as a hyperparameter to set by cross validation. 
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658b0580-ef66-453a-a0f1-cb056ed53bde
- Whileα balances the importance of positive/negative examples, it does not differentiate between easy/hard examples.
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658b065e-5308-47e1-ba96-b5d334035457
- When an example is misclassiﬁed and pt is small, the modulating factor is near 1 and the loss is unaffected. As pt → 1, the factor goes to 0 and the loss for well-classiﬁed examples is down-weighted. 
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658b071e-3b54-4b1a-b367-5e7b3b27666d
- The focusing parameter γ smoothly adjusts the rate at which easy examples are downweighted. When γ = 0, FL is equivalent to CE, and as γ is increased the effect of the modulating factor is likewise increased (we found γ = 2 to work best in our experiments).
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658b0768-2f2c-4f43-a1b8-2e3f939653d6
- Intuitively, the modulating factor reduces the loss contribution from easy examples and extends the range in which an example receives low loss. For instance, with γ = 2, an example classiﬁed with pt = 0.9 would have 100× lower loss compared with CE and with pt ≈ 0.968 it would have1000× lower loss. This in turn increases the importance of correcting misclassiﬁed examples (whose loss is scaled down by at most 4× for pt ≤ .5 and γ = 2).
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658b0864-32e1-44c6-b6de-a6e312996e35
- Finally, we note that the implementation of the loss layer combines the sigmoid operation for computing p with the loss computation, resulting in greater numerical stability
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658b09d2-d587-4d44-8cf1-dfda205f00fc
- Binary classiﬁcation models are by default initialized to have equal probability of outputting either y = −1 or 1. Under such an initialization, in the presence of class imbalance, the loss due to the frequent class can dominate total loss and cause instability in early training.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 658b0f15-df09-45cd-be24-618cb9f37ade
- To counter this, we introduce the concept of a ‘prior’ for the value of p estimated by the model for the rare class (foreground) at the start of training. We denote the prior by π and set it so that the model’s estimated p for examples of the rare class is low, e.g. 0.01. 
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 658b0f2b-4d11-41ce-ad72-00b41875c983
- For the ﬁnal conv layer of the classiﬁcation subnet, we set the bias initialization to b = − log((1 − π)/π), where π speciﬁes that at
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 658b0f78-e435-4f6b-8e19-284b3bf7b73a
- the start of training every anchor should be labeled as foreground with conﬁdence of ∼π. We use π = .01 in all experiments, although results are robust to the exact value. As explained in §3.3, this initialization prevents the large number of background anchors from generating a large, destabilizing loss value in the ﬁrst iteration of training.
  ls-type:: annotation
  hl-page:: 6
  hl-color:: yellow
  id:: 658b0f89-5317-431f-9602-7e0b95c37385
- this to improve training stability for both the cross entropy and focal loss in the case of heavy class imbalance
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 658b1216-65dc-49b3-8e5b-86cbde8d384a
- We note that unlike most recent work, we use a class-agnostic bounding box regressor which uses fewer parameters and we found to be equally effective
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 658b12c3-fdac-42bc-85a8-10ced5fe469b
- We emphasize that when training RetinaNet, the focal loss is applied to all ∼100k anchors in each sampled image. This stands in contrast to common practice of using heuristic sampling (RPN) or hard example mining (OHEM, SSD) to select a small set of anchors (e.g., 256) for each minibatch.
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 658b13a6-6ee2-4bd5-a3df-d52c20160e96
- The total focal loss of an image is computed as the sum of the focal loss over all ∼100k anchors, normalized by the number of anchors assigned to a ground-truth box. We perform the normalization by the number of assigned anchors, not total anchors, since the vast majority of anchors are easy negatives and receive negligible loss values under the focal loss. 
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 658b13b4-b94e-4493-9052-303e69dab50b
- n general α should be decreased slightly as γ is increased (for γ = 2, α = 0.25 works best)
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 658b13d7-f7cb-4634-b6df-2f9fa84856b8
- We observe that lower α’s are selected for higher γ’s (as easy negatives are downweighted, less emphasis needs to be placed on the positives).
  ls-type:: annotation
  hl-page:: 7
  hl-color:: yellow
  id:: 658b1415-7903-4d53-989e-8f5da45680d7
- The training loss is the sum the focal loss and the standard smooth L1 loss used for box regression [10]
  ls-type:: annotation
  hl-page:: 6
  hl-color:: yellow
  id:: 658b1551-ae16-4c1b-a94f-f979a0c28a4b
- Analysis of the Focal Loss:
  ls-type:: annotation
  hl-page:: 7
  hl-color:: yellow
  id:: 658b16a6-dccd-4c9b-9dc3-55af8e5f9741
- Like the focal loss, OHEM puts more emphasis on misclassiﬁed examples, but unlike FL, OHEM completely discards easy examples. We also implement a variant of OHEM used in SSD [22]: after applying nms to all examples, the minibatch is constructed to enforce a 1:3 ratio between positives and negatives to help ensure each minibatch has enough positives.
  ls-type:: annotation
  hl-page:: 7
  hl-color:: yellow
  id:: 658b1827-03cc-4a42-aeb9-f2f8eeb5ae4f
- reduce the contribution of outliers by down-weighting the loss of examples with large errors (hard examples). In contrast, rather than addressing outliers, our focal loss is designed to address class imbalance by down-weighting inliers (easy examples) such that their contribution to the total loss is small even if their number is large. In other words, the focal loss performs the opposite role of a robust loss: it focuses training on a sparse set of hard examples
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658b1c6f-3e07-4d6c-876a-e89c05f898f0
- Finally, in early experiments, we attempted to train with the hinge loss [13] on pt, which sets loss to 0 above a certain value of pt. However, this was unstable and we did not manage to obtain meaningful results.
  ls-type:: annotation
  hl-page:: 7
  hl-color:: yellow
  id:: 658b1ca1-efe3-4bdb-b274-1bf42017a084