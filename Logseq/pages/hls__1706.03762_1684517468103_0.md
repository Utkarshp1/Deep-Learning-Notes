file-path:: ../assets/1706.03762_1684517468103_0.pdf

- [:span]
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 6467c209-d89e-41e7-8962-51ab1b25d198
  hl-type:: area
  hl-stamp:: 1684521481579
- [:span]
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 6467b62f-c46c-4f61-8430-a4fb1a965385
  hl-type:: area
  hl-stamp:: 1684518447107
- Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [ 11 ] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 6467b6ba-3db5-4d2b-8fa1-cf8178e7dce8
- Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 6467b6e3-9f3d-4bb9-a10e-70ed467963f0
- [:span]
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 6467bf80-ee10-4bdb-be12-8b388bea8f6f
  hl-type:: area
  hl-stamp:: 1684520832195
- While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 6467bf02-3d3b-40b4-be7b-c924cb369591
- We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 6467bf17-fa6b-446b-957a-4395f2842f08
- To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q · k = ∑dk i=1 qiki, has mean 0 and variance dk .
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 6467bfc3-e6f2-4ebf-ab37-70928e20a074
- Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 6467c183-bfb6-4abe-8a75-019afae4913a
- We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 6467c1c0-ca65-463b-9676-ad9d65f9b18b
- [:span]
  ls-type:: annotation
  hl-page:: 6
  hl-color:: yellow
  id:: 6467c2dd-4c64-4ba1-ac29-ebdfac8b3383
  hl-type:: area
  hl-stamp:: 1684521693644
- [:span]
  ls-type:: annotation
  hl-page:: 6
  hl-color:: yellow
  id:: 6467c29c-ed86-498e-830a-ecdf51b9e97b
  hl-type:: area
  hl-stamp:: 1684521628180
- To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in6 the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). 
  ls-type:: annotation
  hl-page:: 6
  hl-color:: yellow
  id:: 6467c410-d2b5-41a3-84df-b60972a9e6d1
- A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6 ], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.
  ls-type:: annotation
  hl-page:: 7
  hl-color:: yellow
  id:: 6467c3f6-d653-4069-85a7-4c1ab9253bf7
- [:span]
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 65127b76-e983-4618-87ee-17c138d30b66
  hl-type:: area
  hl-stamp:: 1695710070174