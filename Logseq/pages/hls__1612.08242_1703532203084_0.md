file:: [1612.08242_1703532203084_0.pdf](../assets/1612.08242_1703532203084_0.pdf)
file-path:: ../assets/1612.08242_1703532203084_0.pdf

- Batch normalization also helps regularize the model. With batch normalization we can remove dropout from the model without overﬁtting
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 6589d7db-03bb-4470-b031-603e5900d784
- his means the network has to simultaneously switch to learning object detection and adjust to the new input resolution
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 6589d93c-214f-4863-b5ea-8b6170d178aa
- For YOLOv2 we ﬁrst ﬁne tune the classiﬁcation network at the full 448 × 448 resolution for 10 epochs on ImageNet. This gives the network time to adjust its ﬁlters to work better on higher resolution input. We then ﬁne tune the resulting network on detection.
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 6589d953-575b-46d1-b1ab-f19467f23a33
- Faster R-CNN and SSD both run their proposal networks at various feature maps in the network to get a range of resolutions.
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 6589da36-8a16-4b11-837b-fc0f481e396f
- Our detector runs on top of this expanded feature map so that it has access to ﬁne grained features.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 6589daf4-ea7f-4535-b9bc-9b5bf861c096
- YOLOv2 to be robust to running on images of different sizes so we train this into the model
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 6589db7d-5f56-4394-9c94-88889326e08a
- Instead of ﬁxing the input image size we change the network every few iterations. Every 10 batches our network randomly chooses a new image dimension size. Since our model downsamples by a factor of 32, we pull from the following multiples of 32: {320, 352, ..., 608}. Thus the smallest option is 320 × 320 and the largest is 608 × 608. We resize the network to that dimension and continue training.
  ls-type:: annotation
  hl-page:: 4
  hl-color:: yellow
  id:: 6589db90-12f9-4bbe-add5-e840cb8b1e44
- Predicting offsets instead of coordinates simpliﬁes the problem and makes it easier for the network to learn.
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 658ad08e-d82f-4e6a-b633-370ee1779da0
- We also shrink the network to operate on 416 input images instead of 448×448. We do this because we want an odd number of locations in our feature map so there is a single center cell. Objects, especially large objects, tend to occupy the center of the image so it’s good to have a single location right at the center to predict these objects instead of four locations that are all nearby.
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 658ad0d7-580d-46bd-ac28-79be153be2dd
- When we move to anchor boxes we also decouple the class prediction mechanism from the spatial location and instead predict class and objectness for every anchor box. Following YOLO, the objectness prediction still predicts the IOU of the ground truth and the proposed box and the class predictions predict the conditional probability of that class given that there is an object.
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 658ad0ec-803a-49ca-824f-4ed31314e1c0
- Without anchor boxes our intermediate model gets 69.5 mAP with a recall of 81%. With anchor boxes our model gets 69.2 mAP with a recall of 88%.
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 658ad16a-e60e-4eff-82cb-6f07092ad067
- Instead of choosing priors by hand, we run k-means clustering on the training set bounding boxes to automat-
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 658ad1d6-c3a1-43f9-9e65-2c6917f03687
- ically ﬁnd good priors. If we use standard k-means with Euclidean distance larger boxes generate more error than smaller boxes. However, what we really want are priors that lead to good IOU scores, which is independent of the size of the box. Thus for our distance metric we use
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658ad1e0-46d7-4434-bd39-5574279589b2
- We choose k = 5 as a good tradeoff between model complexity and high recall. The cluster centroids are signiﬁcantly different than hand-picked anchor boxes. There are fewer short, wide boxes and more tall, thin boxes.
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658ad237-7b71-4e41-b712-11a72c0b1727
- At only 5 priors the centroids perform similarly to 9 anchor boxes with an average IOU of 61.0 compared to 60.9.
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658ad358-97ec-40f8-b47e-45214655dae2
- This formulation is unconstrained so any anchor box can end up at any point in the image, regardless of what location predicted the box. With random initialization the model takes a long time to stabilize to predicting sensible offsets.
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658ad5d3-fb29-46f6-a5c3-180875f59570
- Instead of predicting offsets we follow the approach of YOLO and predict location coordinates relative to the location of the grid cell. This bounds the ground truth to fall between 0 and 1. We use a logistic activation to constrain the network’s predictions to fall in this range.
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658ad5f5-9a93-434e-873c-33e8f30f7173
- [:span]
  ls-type:: annotation
  hl-page:: 3
  hl-color:: yellow
  id:: 658ad612-a912-4f1f-b26d-bcd5652c68da
  hl-type:: area
  hl-stamp:: 1703597585710
- [:span]
  ls-type:: annotation
  hl-page:: 5
  hl-color:: yellow
  id:: 658ad6ee-47b5-47eb-ba12-0c78ae3f22b6
  hl-type:: area
  hl-stamp:: 1703597805889